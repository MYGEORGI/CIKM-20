{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence, Token\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import stanfordnlp\n",
    "import nltk\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_WIKIPEDIA_EXTRACTION = \"../../data/causality-graphs/extraction/\"\n",
    "PATH_WIKIPEDIA_EXTRACTION += \"wikipedia/wikipedia-extraction.tsv-backup\"\n",
    "PATH_FLAIR_FOLDER = \"../../data/flair-models/infoboxes/\"\n",
    "\n",
    "PATH_NLTK_RESOURCES = \"../../data/external/nltk/\"\n",
    "PATH_STANFORD_RESOURCES = \"../../data/external/stanfordnlp/\"\n",
    "\n",
    "PATH_OUTPUT_GRAPH = \"../../data/causality-graphs/spotting/\"\n",
    "PATH_OUTPUT_GRAPH += \"wikipedia/infobox-graph.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../../data/external/nltk/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ../../data/external/nltk/...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt', PATH_NLTK_RESOURCES)\n",
    "nltk.download('averaged_perceptron_tagger', PATH_NLTK_RESOURCES)\n",
    "nltk.data.path.append(PATH_NLTK_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Use device: gpu\n",
      "\tTorch-GPU-ID: 0\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en', PATH_STANFORD_RESOURCES)\n",
    "stanford_nlp = stanfordnlp.Pipeline(processors='tokenize,pos',\n",
    "                                    models_dir=PATH_STANFORD_RESOURCES,\n",
    "                                    treebank='en_ewt',\n",
    "                                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading infobox data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_set = []\n",
    "\n",
    "for line in open(PATH_WIKIPEDIA_EXTRACTION, encoding=\"utf-8\"):\n",
    "    parts = line.strip().split('\\t')\n",
    "    if parts[0] != 'wikipedia_infobox':\n",
    "        continue\n",
    "\n",
    "    assert len(parts) == 9\n",
    "    infobox_data = {\n",
    "        \"value\": json.loads(parts[8]),\n",
    "        \"type\": \"wikipedia_infobox\",\n",
    "        \"payload\": {\n",
    "            \"wikipedia_page_id\": parts[1],\n",
    "            \"wikipedia_page_title\": parts[2],\n",
    "            \"wikipedia_revision_id\": parts[3],\n",
    "            \"wikipedia_revision_timestamp\": parts[4],\n",
    "            \"infobox_template\": parts[5][1:-1],\n",
    "            \"infobox_title\": parts[6][1:-1],\n",
    "            \"infobox_argument\": parts[7][1:-1]\n",
    "        }\n",
    "    }\n",
    "    infobox_set.append(infobox_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_article(title):\n",
    "    forbidden_title_parts = ['Wikipedia:', 'Template:', 'File:',\n",
    "                             'Portal:', 'Category:', 'Draft:',\n",
    "                             'List of', 'disambiguation']\n",
    "\n",
    "    contains_forbidden_title_part = False\n",
    "    for forbidden_title_part in forbidden_title_parts:\n",
    "        if forbidden_title_part in title:\n",
    "            contains_forbidden_title_part = True\n",
    "            break\n",
    "\n",
    "    return not contains_forbidden_title_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_causal_infobox(infobox_template):\n",
    "    not_causal = ['infobox former arab villages in palestine',\n",
    "                  'infobox soap character',\n",
    "                  'infobox serial killer',\n",
    "                  'infobox criminal',\n",
    "                  'infobox mass murderer',\n",
    "                  'infobox murderer']\n",
    "    return infobox_template.lower() not in not_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_data_filtered = []\n",
    "statistics = {}\n",
    "\n",
    "for infobox in infobox_set:\n",
    "    if not is_valid_article(infobox['payload']['wikipedia_page_title']):\n",
    "        continue\n",
    "\n",
    "    if not is_causal_infobox(infobox['payload']['infobox_template']):\n",
    "        continue\n",
    "\n",
    "    if infobox['payload']['infobox_argument'] == \"result\":\n",
    "        continue\n",
    "\n",
    "    template = infobox['payload']['infobox_template'].lower()\n",
    "    page_id = infobox['payload']['wikipedia_page_id']\n",
    "    statistics.setdefault(template, []).append(page_id)\n",
    "\n",
    "    infobox_data_filtered.append(infobox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'infobox birth control',\n",
       " 'infobox bus accident',\n",
       " 'infobox civil conflict',\n",
       " 'infobox event',\n",
       " 'infobox medical condition (new)',\n",
       " 'infobox military conflict',\n",
       " 'infobox news event',\n",
       " 'infobox oil spill',\n",
       " 'infobox rail accident',\n",
       " 'infobox wildfire'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "considered_infoboxes = []\n",
    "\n",
    "for template in sorted(statistics.items(),\n",
    "                       key=lambda statistics: len(set(statistics[1])),\n",
    "                       reverse=True):\n",
    "    if len(set(template[1])) >= 10:\n",
    "        considered_infoboxes.append(template[0])\n",
    "\n",
    "considered_infoboxes = set(considered_infoboxes)\n",
    "considered_infoboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "\n",
    "    # remove newlines\n",
    "    string = '; '.join([s.strip()\n",
    "                        for s in string.split('\\n')\n",
    "                        if s.strip() != ''])\n",
    "\n",
    "    # remove brackets\n",
    "    string = re.sub(r\"\\s?\\(.*\\)\\s?\", \"\", string).strip()\n",
    "\n",
    "    # Remove leading list headings like \"Initially: ... Later: ...\"\n",
    "    # heading at beginning\n",
    "    string = re.sub(r\"^\\w+:\\s?\", \"\", string).strip()\n",
    "    # heading within the string\n",
    "    string = re.sub(r\"\\w+:\\s?\", \"\", string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoboxes_for_spotting = []\n",
    "\n",
    "for infobox in infobox_data_filtered:\n",
    "    template = infobox['payload']['infobox_template'].lower()\n",
    "    if template not in considered_infoboxes:\n",
    "        continue\n",
    "\n",
    "    infobox['value'] = preprocess(infobox['value'])\n",
    "\n",
    "    if len(infobox['value']) == 0:\n",
    "        continue\n",
    "\n",
    "    result = {'value': infobox['value'],\n",
    "              \"sources\": [{'type': infobox['type'],\n",
    "                           'payload': infobox['payload']}]}\n",
    "    infoboxes_for_spotting.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_of_tags_in_sentence(sentence, tags):\n",
    "    # go from left to right and determine tag offsets\n",
    "    offsets = []\n",
    "    total_offset = 0\n",
    "    for tag in tags:\n",
    "        label = tag[0]\n",
    "        local_offset = sentence.find(label)\n",
    "        offset = total_offset + local_offset\n",
    "        offsets.append(offset)\n",
    "\n",
    "        # prepare for next iteration\n",
    "        sentence = sentence[local_offset + len(label):]\n",
    "        total_offset = offset + len(label)\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def get_pos_tags_of_sentence(sentence):\n",
    "    tags = []\n",
    "    for token in sentence.tokens:\n",
    "        for word in token.words:\n",
    "            tags.append((word.text, word.pos))\n",
    "    return tags\n",
    "\n",
    "\n",
    "def calculate_pos_tags_for_string(doc):\n",
    "    tags = []\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_pos = []\n",
    "        for pos in get_pos_tags_of_sentence(sentence):\n",
    "            sentence_pos.append(pos)\n",
    "        tags.append(sentence_pos)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def pos_tagging(sentence):\n",
    "    doc = stanford_nlp(sentence)\n",
    "    tags = calculate_pos_tags_for_string(doc)[0]\n",
    "    offsets = get_offset_of_tags_in_sentence(sentence, tags)\n",
    "    return [(tags[x][0], tags[x][1], str(offsets[x])) for x in range(len(tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in infoboxes_for_spotting:\n",
    "    sample['value:POS'] = pos_tagging(sample['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infobox Spotter: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(chunk):\n",
    "    return [int(chunk.tokens[0].get_tag('idx').value),\n",
    "            int(chunk.tokens[-1].get_tag('idx').value)\n",
    "            + len(chunk.tokens[-1].text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-16 11:43:29,318 loading file ../../data/flair-models/infoboxes/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = SequenceTagger.load(PATH_FLAIR_FOLDER + 'final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in infoboxes_for_spotting:\n",
    "    sentence = Sentence(use_tokenizer=False)\n",
    "\n",
    "    for pos in sample['value:POS']:\n",
    "        token = Token(pos[0])\n",
    "        token.add_tag('POS', pos[1])\n",
    "        token.add_tag('idx', pos[2])\n",
    "        sentence.add_token(token)\n",
    "\n",
    "    classifier.predict(sentence)\n",
    "    chunks = [get_index(chunk) for chunk in sentence.get_spans('chunk_BIO')]\n",
    "\n",
    "    extraction = []\n",
    "    for chunk in chunks:\n",
    "        extraction.append(sample['value'][chunk[0]:chunk[1]])\n",
    "\n",
    "    sample['extraction'] = extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(page_title, infobox_title,\n",
    "                 infobox_type, infobox_argument, value):\n",
    "    if infobox_title != 'None':\n",
    "        subject = infobox_title\n",
    "    else:\n",
    "        subject = page_title\n",
    "\n",
    "    if infobox_argument == 'symptoms':\n",
    "        return (subject, value)\n",
    "\n",
    "    if infobox_argument in ['cause', 'causes']:\n",
    "        return (value, subject)\n",
    "\n",
    "    if infobox_argument == 'risks':\n",
    "        if infobox_type.lower() in ['infobox medical condition (new)']:\n",
    "            return (value, subject)\n",
    "        elif infobox_type.lower() in ['infobox birth control']:\n",
    "            return (subject, value)\n",
    "\n",
    "    raise Exception(\"Not handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_of_tags_in_sentence(sentence, tags):\n",
    "    # go from left to right and determine tag offsets\n",
    "    offsets = []\n",
    "    total_offset = 0\n",
    "    for tag in tags:\n",
    "        label = tag[0]\n",
    "        local_offset = sentence.find(label)\n",
    "        offset = total_offset + local_offset\n",
    "        offsets.append(offset)\n",
    "\n",
    "        # prepare for next iteration\n",
    "        sentence = sentence[local_offset + len(label):]\n",
    "        total_offset = offset + len(label)\n",
    "\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def post_process_value(value):\n",
    "    tags = pos_tag(word_tokenize(value))\n",
    "    offset = get_offset_of_tags_in_sentence(value, tags)\n",
    "\n",
    "    if len(tags) == 1:\n",
    "        return value\n",
    "\n",
    "    left = 0\n",
    "    right = len(tags)-1\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag[1] in ['CC', 'DT']:\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for tag in reversed(tags):\n",
    "        if tag[1] in ['.', ',', ';', 'CC']:\n",
    "            right -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return value[offset[left]:offset[right] + len(tags[right][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_graph = []\n",
    "\n",
    "for sample in infoboxes_for_spotting:\n",
    "    for value in sample['extraction']:\n",
    "        value = post_process_value(value)\n",
    "\n",
    "        if len(value) == 0:\n",
    "            continue\n",
    "\n",
    "        source = sample['sources'][0]['payload']\n",
    "        relation = get_relation(source['wikipedia_page_title'],\n",
    "                                source['infobox_title'],\n",
    "                                source['infobox_template'],\n",
    "                                source['infobox_argument'],\n",
    "                                value)\n",
    "\n",
    "        causal_relation = {'causal_relation': {\n",
    "            'cause': {'concept': relation[0]},\n",
    "            'effect': {'concept': relation[1]},\n",
    "        }, 'sources': sample['sources']}\n",
    "        infobox_graph.append(causal_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Infobox-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonarray = json.dumps(infobox_graph)\n",
    "file_list_graph = open(PATH_OUTPUT_GRAPH, \"w+\")\n",
    "file_list_graph.write(jsonarray)\n",
    "file_list_graph.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cikm20-causenet",
   "language": "python",
   "name": "cikm20-causenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
