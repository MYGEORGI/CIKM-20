{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence, Token\n",
    "import stanfordnlp\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CLUEWEB_EXTRACTION = \"../../data/causality-graphs/extraction/\"\n",
    "PATH_CLUEWEB_EXTRACTION += \"clueweb12/clueweb12-extraction.tsv\"\n",
    "PATH_FLAIR_FOLDER = \"../../data/flair-models/sentences/\"\n",
    "\n",
    "PATH_STANFORD_RESOURCES = \"../../data/external/stanfordnlp/\"\n",
    "\n",
    "PATH_OUTPUT_GRAPH = \"../../data/causality-graphs/spotting/\"\n",
    "PATH_OUTPUT_GRAPH += \"clueweb12/clueweb-graph.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Use device: gpu\n",
      "\tTorch-GPU-ID: 0\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'pretokenized': True, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en', PATH_STANFORD_RESOURCES)\n",
    "stanford_nlp = stanfordnlp.Pipeline(processors='tokenize,pos',\n",
    "                                    tokenize_pretokenized=True,\n",
    "                                    models_dir=PATH_STANFORD_RESOURCES,\n",
    "                                    treebank='en_ewt',\n",
    "                                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading sentence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_set = []\n",
    "\n",
    "for line in open(PATH_CLUEWEB_EXTRACTION, encoding=\"utf-8\"):\n",
    "    parts = line.strip().split('\\t')\n",
    "    if parts[0] != 'clueweb12_sentence':\n",
    "        continue\n",
    "    assert len(parts) == 8\n",
    "    \n",
    "    for match in json.loads(parts[7]):\n",
    "        sentence_data = {\n",
    "            \"causal_relation\": match,\n",
    "            \"sources\": [{\n",
    "                \"type\": \"clueweb12_sentence\",\n",
    "                \"payload\": {\n",
    "                    \"clueweb12_page_id\": parts[1],\n",
    "                    \"clueweb12_page_reference\": parts[2],\n",
    "                    \"clueweb12_page_timestamp\": parts[3],\n",
    "                    \"sentence\": {\n",
    "                        \"surface\": json.loads(parts[4]),\n",
    "                        \"tokens\": json.loads(parts[5]),\n",
    "                        \"dependencies\": json.loads(parts[6])\n",
    "                        },\n",
    "                    \"path_pattern\": match['Pattern']\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        sentence_set.append(sentence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_of_tags_in_sentence(sentence, tags):\n",
    "    # go from left to right and determine tag offsets\n",
    "    offsets = []\n",
    "    total_offset = 0\n",
    "    for tag in tags:\n",
    "        label = tag[0]\n",
    "        local_offset = sentence.find(label)\n",
    "        offset = total_offset + local_offset\n",
    "        offsets.append(offset)\n",
    "\n",
    "        # prepare for next iteration\n",
    "        sentence = sentence[local_offset + len(label):]\n",
    "        total_offset = offset + len(label)\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def get_pos_tags_of_sentence(sentence):\n",
    "    tags = []\n",
    "    for token in sentence.tokens:\n",
    "        for word in token.words:\n",
    "            tags.append((word.text, word.pos))\n",
    "    return tags\n",
    "\n",
    "\n",
    "def calculate_pos_tags_for_string(doc):\n",
    "    tags = []\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_pos = []\n",
    "        for pos in get_pos_tags_of_sentence(sentence):\n",
    "            sentence_pos.append(pos)\n",
    "        tags.append(sentence_pos)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(sentences_to_predict):\n",
    "    batch_of_tokens = [sample['sources'][0]['payload']['sentence']['tokens']\n",
    "                       for sample in sentences_to_predict]\n",
    "    strings = [' '.join(tokens) for tokens in batch_of_tokens]\n",
    "\n",
    "    # batch processing is faster\n",
    "    batch = '\\n\\n'.join(strings)\n",
    "    doc = stanford_nlp(batch)\n",
    "    tags = calculate_pos_tags_for_string(doc)\n",
    "\n",
    "    assert len(tags) == len(sentences_to_predict)\n",
    "\n",
    "    for i in range(len(sentences_to_predict)):\n",
    "        sample = sentences_to_predict[i]\n",
    "        sentence = sample['sources'][0]['payload']['sentence']['surface']\n",
    "        offsets = get_offset_of_tags_in_sentence(sentence, tags[i])\n",
    "        sample['sources'][0]['payload']['sentence']['POS'] = [\n",
    "            (tags[i][x][0], tags[i][x][1], str(offsets[x]))\n",
    "            for x in range(len(tags[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Spotter: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(batch):\n",
    "    sentences = []\n",
    "\n",
    "    for sample in batch:\n",
    "        sentence = Sentence(use_tokenizer=False)\n",
    "\n",
    "        tokens = sample['sources'][0]['payload']['sentence']['tokens']\n",
    "        POS_tags = sample['sources'][0]['payload']['sentence']['POS']\n",
    "        if len(tokens) > 200:\n",
    "            # skipping sentences with too many tokens\n",
    "            # due to GPU memory limitation\n",
    "            continue\n",
    "\n",
    "        for pos in POS_tags:\n",
    "            token = Token(pos[0])\n",
    "            token.add_tag('POS', pos[1])\n",
    "            token.add_tag('idx', pos[2])\n",
    "            sentence.add_token(token)\n",
    "\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences, mini_batches):\n",
    "    prediction = []\n",
    "    classifier.predict(sentences, mini_batches)\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        indices = [[token.idx-1 for token in chunk.tokens]\n",
    "                   for chunk in sentence.get_spans('chunk_BIO')]\n",
    "\n",
    "        extraction = []\n",
    "        for index_list in indices:\n",
    "            result = [sentence.tokens[index].text\n",
    "                      for index in index_list]\n",
    "            extraction.append(' '.join(result))\n",
    "\n",
    "        prediction.append([extraction, indices])\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(relation, indices):\n",
    "    cause_index = int(relation['Cause'][1])\n",
    "    effect_index = int(relation['Effect'][1])\n",
    "\n",
    "    cause_match = None\n",
    "    effect_match = None\n",
    "\n",
    "    for index_range in indices:\n",
    "        if cause_index in index_range:\n",
    "            cause_match = indices.index(index_range)\n",
    "        if effect_index in index_range:\n",
    "            effect_match = indices.index(index_range)\n",
    "\n",
    "        if (cause_match is not None\n",
    "                and effect_match is not None\n",
    "                and cause_match != effect_match):\n",
    "            return [cause_match, effect_match]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_relations(batch, prediction):\n",
    "    relations = []\n",
    "    skipped_elements = 0\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        sample = batch[i]\n",
    "        tokens = sample['sources'][0]['payload']['sentence']['tokens']\n",
    "        POS_tags = sample['sources'][0]['payload']['sentence']['POS']\n",
    "        if len(tokens) > 200:\n",
    "            # skipping sentences with too many tokens\n",
    "            # due to GPU memory limitation\n",
    "            # see method prepare(batch)\n",
    "            skipped_elements += 1\n",
    "            continue\n",
    "\n",
    "        path_pattern_extraction = sample['causal_relation']\n",
    "        spotting_extraction, indices = prediction[i - skipped_elements]\n",
    "\n",
    "        match = find_match(path_pattern_extraction, indices)\n",
    "        if len(match) < 2:\n",
    "            # In cases the tagger failed,\n",
    "            # we disregarded the causal concepts\n",
    "            continue\n",
    "        cause_match, effect_match = match\n",
    "\n",
    "        cause = spotting_extraction[cause_match]\n",
    "        effect = spotting_extraction[effect_match]\n",
    "\n",
    "        # concept POS (save for later post-processing)\n",
    "        cause_pos_raw = [POS_tags[j] for j in indices[cause_match]]\n",
    "        offset = get_offset_of_tags_in_sentence(cause, cause_pos_raw)\n",
    "        cause_pos = [(cause_pos_raw[x][0],\n",
    "                      cause_pos_raw[x][1],\n",
    "                      str(offset[x]))\n",
    "                     for x in range(len(cause_pos_raw))]\n",
    "\n",
    "        effect_pos_raw = [POS_tags[j] for j in indices[effect_match]]\n",
    "        offset = get_offset_of_tags_in_sentence(effect, effect_pos_raw)\n",
    "        effect_pos = [(effect_pos_raw[x][0],\n",
    "                       effect_pos_raw[x][1],\n",
    "                       str(offset[x]))\n",
    "                      for x in range(len(effect_pos_raw))]\n",
    "\n",
    "        causal_relation = {'causal_relation': {\n",
    "            'cause': {'concept': cause, 'POS': cause_pos},\n",
    "            'effect': {'concept': effect, 'POS': effect_pos},\n",
    "        }, 'sources': sample['sources']}\n",
    "        relations.append(causal_relation)\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-16 11:42:33,845 loading file ../../data/flair-models/sentences/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = SequenceTagger.load(PATH_FLAIR_FOLDER + 'final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_graph = []\n",
    "\n",
    "batch_size = 512 if len(sentence_set) > 512 else 32\n",
    "batches = np.array_split(sentence_set, len(sentence_set)/batch_size)\n",
    "\n",
    "for i in range(len(batches)):\n",
    "    batch = batches[i]\n",
    "    pos_tagging(batch)\n",
    "    prepared_sentences = prepare(batch)\n",
    "    prediction = predict(prepared_sentences, mini_batches=32)\n",
    "    batch_relations = get_relations(batch, prediction)\n",
    "\n",
    "    for relation in batch_relations:\n",
    "        text_graph.append(relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(value, pos_tags):\n",
    "    left = 0\n",
    "    right = len(pos_tags)-1\n",
    "\n",
    "    punctuation = ['.', ',', ';', '(', ')', '``', \"''\"]\n",
    "    cutoff = ['CC', 'DT', 'PRP', 'PRP$'] + punctuation\n",
    "\n",
    "    for tag in pos_tags:\n",
    "        if tag[1] in cutoff:\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for tag in reversed(pos_tags):\n",
    "        if tag[1] in cutoff:\n",
    "            right -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return value[int(pos_tags[left][2]):int(pos_tags[right][2])\n",
    "                 + len(pos_tags[right][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in text_graph:\n",
    "    cause_concept = relation['causal_relation']['cause']['concept']\n",
    "    cause_pos = relation['causal_relation']['cause']['POS']\n",
    "    cause = post_process(cause_concept, cause_pos)\n",
    "\n",
    "    effect_concept = relation['causal_relation']['effect']['concept']\n",
    "    effect_pos = relation['causal_relation']['effect']['POS']\n",
    "    effect = post_process(effect_concept, effect_pos)\n",
    "\n",
    "    causal_relation = {\n",
    "        'cause': {'concept': cause},\n",
    "        'effect': {'concept': effect}\n",
    "    }\n",
    "    relation['causal_relation'] = causal_relation\n",
    "\n",
    "    # further cleanup\n",
    "    del relation['sources'][0]['payload']['sentence']['POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Text-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonarray = json.dumps(text_graph)\n",
    "file_list_graph = open(PATH_OUTPUT_GRAPH, \"w+\")\n",
    "file_list_graph.write(jsonarray)\n",
    "file_list_graph.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cikm20-causenet",
   "language": "python",
   "name": "cikm20-causenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
