{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence, Token\n",
    "from nltk import word_tokenize\n",
    "import stanfordnlp\n",
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_WIKIPEDIA_EXTRACTION = \"../../data/causality-graphs/extraction/\"\n",
    "PATH_WIKIPEDIA_EXTRACTION += \"wikipedia/wikipedia-extraction.tsv-backup\"\n",
    "PATH_FLAIR_FOLDER = \"../../data/flair-models/lists/\"\n",
    "\n",
    "PATH_NLTK_RESOURCES = \"../../data/external/nltk/\"\n",
    "PATH_STANFORD_RESOURCES = \"../../data/external/stanfordnlp/\"\n",
    "\n",
    "PATH_OUTPUT_GRAPH = \"../../data/causality-graphs/spotting/wikipedia/list-graph.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     ../../data/downloads/nltk_data/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt', PATH_NLTK_RESOURCES)\n",
    "nltk.data.path.append(PATH_NLTK_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Use device: gpu\n",
      "\tTorch-GPU-ID: 0\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "Use device: gpu\n",
      "\tTorch-GPU-ID: 0\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'pretokenized': True, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '../../data/downloads/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en', PATH_STANFORD_RESOURCES)\n",
    "stanford_nlp = stanfordnlp.Pipeline(processors='tokenize,pos',\n",
    "                                    models_dir=PATH_STANFORD_RESOURCES,\n",
    "                                    treebank='en_ewt',\n",
    "                                    use_gpu=True)\n",
    "\n",
    "stanford_nlp_pretokenized = stanfordnlp.Pipeline(processors='tokenize,pos',\n",
    "                                    models_dir=PATH_STANFORD_RESOURCES,\n",
    "                                    treebank='en_ewt',\n",
    "                                    tokenize_pretokenized=True,\n",
    "                                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.8\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Spacy version: 2.1.8\n",
    "# Model version: 2.1.0\n",
    "# pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(spacy.__version__)\n",
    "print(spacy_nlp.meta['version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_set = []\n",
    "\n",
    "for line in open(PATH_WIKIPEDIA_EXTRACTION, encoding=\"utf-8\"):\n",
    "    parts = line.strip().split('\\t')\n",
    "    if parts[0] != 'wikipedia_list':\n",
    "        continue\n",
    "    assert len(parts) == 9\n",
    "    list_data = {\n",
    "        \"list\": json.loads(parts[8]),\n",
    "        \"type\": \"wikipedia_list\",\n",
    "        \"payload\": {\n",
    "            \"wikipedia_page_id\": parts[1],\n",
    "            \"wikipedia_page_title\": parts[2],\n",
    "            \"wikipedia_revision_id\": parts[3],\n",
    "            \"wikipedia_revision_timestamp\": parts[4],\n",
    "            \"list_toc_parent_title\": parts[5][1:-1],\n",
    "            \"list_toc_section_heading\": parts[6][1:-1],\n",
    "            \"list_toc_section_level\": parts[7]\n",
    "        }\n",
    "    }\n",
    "    list_set.append(list_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_article(title):\n",
    "    forbidden_title_parts = ['Wikipedia:', 'Template:', 'File:',\n",
    "                             'Portal:', 'Category:', 'Draft:',\n",
    "                             'List of', 'disambiguation']\n",
    "\n",
    "    contains_forbidden_title_part = False\n",
    "    for forbidden_title_part in forbidden_title_parts:\n",
    "        if forbidden_title_part in title:\n",
    "            contains_forbidden_title_part = True\n",
    "            break\n",
    "\n",
    "    return not contains_forbidden_title_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_causal_section(section_title, section_level):\n",
    "    allowed_sections = ['Cause', 'Causes', 'Risk factor',\n",
    "                        'Risk factors' 'Symptom', 'Symptoms',\n",
    "                        'Signs and symptoms']\n",
    "    return section_title in allowed_sections and section_level == '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_preprocessed = []\n",
    "\n",
    "for list_data in list_set:\n",
    "    if not is_valid_article(list_data['payload']['wikipedia_page_title']):\n",
    "        continue\n",
    "\n",
    "    list_toc_section_heading = list_data['payload']['list_toc_section_heading']\n",
    "    list_toc_section_level = list_data['payload']['list_toc_section_level']\n",
    "    if not is_in_causal_section(list_toc_section_heading,\n",
    "                                list_toc_section_level):\n",
    "        continue\n",
    "\n",
    "    list_data_preprocessed.append(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(sentence):\n",
    "    tags = []\n",
    "    for token in sentence.tokens:\n",
    "        for word in token.words:\n",
    "            tags.append((word.text, word.pos))\n",
    "    return tags\n",
    "\n",
    "\n",
    "def pos_tagging(doc):\n",
    "    tags = []\n",
    "    for sentence in doc.sentences:\n",
    "        for pos in get_pos(sentence):\n",
    "            tags.append(pos)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_verb(pos_tag):\n",
    "    '''\n",
    "    List of verb-POS-Tags:\n",
    "    VB - Verb, base form\n",
    "    VBD - Verb, past tense\n",
    "    VBG - Verb, gerund or present participle\n",
    "    VBN - Verb, past participle\n",
    "    VBP - Verb, non-3rd person singular present\n",
    "    VBZ - Verb, 3rd person singular present\n",
    "    '''\n",
    "    return pos_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    # split newlines\n",
    "    newline_split = [elem.strip()\n",
    "                     for elem in string.split('\\n')\n",
    "                     if elem.strip() != '']\n",
    "\n",
    "    # remove brackets\n",
    "    for i in range(len(newline_split)):\n",
    "        newline_split[i] = re.sub(r\"\\s?\\([^)]*\\)\\s?\",\n",
    "                                  \"\",\n",
    "                                  newline_split[i]).strip()\n",
    "\n",
    "    '''\n",
    "        - remove 'See ...'\n",
    "        - remove paragraphs\n",
    "        - (multiple sentences; contains more than a sentence -> is paragraph)\n",
    "        - remove whole sentences (phrases with verb)\n",
    "    '''\n",
    "    result = []\n",
    "    for string in newline_split:\n",
    "\n",
    "        # Remove 'See ...'\n",
    "        if re.match(r\"^See\\s.*\", string) is not None:\n",
    "            continue\n",
    "\n",
    "        # Remove leading list headings like \"Initially: ... Later: ...\"\n",
    "        # heading at beginning\n",
    "        string = re.sub(r\"^\\w+:\\s?\", \"\", string).strip()\n",
    "        # heading within the string\n",
    "        string = re.sub(r\"\\w+:\\s?\", \"\", string).strip()\n",
    "\n",
    "        if string is None or string == '':\n",
    "            continue\n",
    "\n",
    "        doc = stanford_nlp(string)\n",
    "\n",
    "        # Remove paragraphs\n",
    "        if len(doc.sentences) != 1:\n",
    "            continue\n",
    "\n",
    "        # Remove whole sentences (with verb)\n",
    "        contains_verb = False\n",
    "        pos_tags = pos_tagging(doc)\n",
    "        for i in range(len(pos_tags)):\n",
    "            if is_verb(pos_tags[i][1]):\n",
    "                contains_verb = True\n",
    "                break\n",
    "        if contains_verb:\n",
    "            continue\n",
    "\n",
    "        result.append(string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_data in list_data_preprocessed:\n",
    "    list_data['list'] = preprocess(list_data['list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_for_spotting = []\n",
    "for list_data in list_data_preprocessed:\n",
    "    for data in list_data['list']:\n",
    "        result = {'list': data,\n",
    "                  \"sources\": [{'type': list_data['type'],\n",
    "                               'payload': list_data['payload']}]}\n",
    "        list_data_for_spotting.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_of_tags_in_sentence(sentence, tags):\n",
    "    # go from left to right and determine tag offsets\n",
    "    offsets = []\n",
    "    total_offset = 0\n",
    "    for tag in tags:\n",
    "        label = tag[0]\n",
    "        local_offset = sentence.find(label)\n",
    "        offset = total_offset + local_offset\n",
    "        offsets.append(offset)\n",
    "\n",
    "        # prepare for next iteration\n",
    "        sentence = sentence[local_offset + len(label):]\n",
    "        total_offset = offset + len(label)\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def get_pos_tags_of_sentence(sentence):\n",
    "    tags = []\n",
    "    for token in sentence.tokens:\n",
    "        for word in token.words:\n",
    "            tags.append((word.text, word.pos))\n",
    "    return tags\n",
    "\n",
    "\n",
    "def calculate_pos_tags_for_string(doc):\n",
    "    tags = []\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_pos = []\n",
    "        for pos in get_pos_tags_of_sentence(sentence):\n",
    "            sentence_pos.append(pos)\n",
    "        tags.append(sentence_pos)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def pos_tagging(strings):\n",
    "    batch = '\\n\\n'.join([' '.join(word_tokenize(string))\n",
    "                         for string in strings])\n",
    "    doc = stanford_nlp_pretokenized(batch)\n",
    "    tags = calculate_pos_tags_for_string(doc)\n",
    "    assert len(tags) == len(strings)\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(strings)):\n",
    "        offsets = get_offset_of_tags_in_sentence(strings[i], tags[i])\n",
    "        result.append([(tags[i][x][0], tags[i][x][1], str(offsets[x]))\n",
    "                       for x in range(len(tags[i]))])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = []\n",
    "for sample in list_data_for_spotting:    \n",
    "    strings.append(sample['list'])\n",
    "tags = pos_tagging(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_data_for_spotting)):\n",
    "    sample = list_data_for_spotting[i]\n",
    "    sample['list:POS'] = tags[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List-Spotter: Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(chunk):\n",
    "    return [int(chunk.tokens[0].get_tag('idx').value),\n",
    "            int(chunk.tokens[-1].get_tag('idx').value)\n",
    "            + len(chunk.tokens[-1].text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-14 21:58:24,028 loading file ../../data/flair-data/lists/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = SequenceTagger.load(PATH_FLAIR_FOLDER + 'final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in list_data_for_spotting:\n",
    "    sentence = Sentence(use_tokenizer=False)\n",
    "\n",
    "    for pos in sample['list:POS']:\n",
    "        token = Token(pos[0])\n",
    "        token.add_tag('POS', pos[1])\n",
    "        token.add_tag('idx', pos[2])\n",
    "        sentence.add_token(token)\n",
    "    \n",
    "    classifier.predict(sentence)\n",
    "    chunks = [get_index(chunk) for chunk in sentence.get_spans('chunk_BIO')]\n",
    "\n",
    "    extraction = []\n",
    "    for chunk in chunks:\n",
    "        extraction.append(sample['list'][chunk[0]:chunk[1]])\n",
    "\n",
    "    sample['extraction'] = extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def spacy_tagging(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    tagging = []\n",
    "    for token in doc:\n",
    "        tagging.append((token.text, token.tag_, token.idx))\n",
    "    return tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def post_process_value(value):\n",
    "    if value is None:\n",
    "        return value\n",
    "\n",
    "    tags = spacy_tagging(value)\n",
    "\n",
    "    left = 0\n",
    "    right = len(tags)-1\n",
    "\n",
    "    punctuation = ['.', ',', ';', '(', ')', '``', \"''\"]\n",
    "    cutoff = ['CC', 'DT', 'PRP', 'PRP$'] + punctuation\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag[1] in cutoff:\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for tag in reversed(tags):\n",
    "        if tag[1] in cutoff:\n",
    "            right -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        return value[tags[left][2]:tags[right][2] + len(tags[right][0])]\n",
    "    except:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_relation(section_heading, parent_title, value):\n",
    "\n",
    "    if section_heading in ['Symptom', 'Symptoms', 'Signs and symptoms']:\n",
    "        return (parent_title, value)\n",
    "\n",
    "    if section_heading in ['Cause', 'Causes']:\n",
    "        return (value, parent_title)\n",
    "\n",
    "    if section_heading in ['Risk factor', 'Risk factors']:\n",
    "        return (value, parent_title)\n",
    "\n",
    "    raise Exception(\"Not handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_graph = []\n",
    "\n",
    "for sample in list_data_for_spotting:\n",
    "    for value in sample['extraction']:\n",
    "        value = post_process_value(value)\n",
    "\n",
    "        if len(value) == 0:\n",
    "            continue\n",
    "\n",
    "        source = sample['sources'][0]['payload']\n",
    "        relation = get_relation(\n",
    "            source['list_toc_section_heading'],\n",
    "            source['list_toc_parent_title'],\n",
    "            value)\n",
    "\n",
    "        causal_relation = {'causal_relation': {\n",
    "            'cause': {'concept': relation[0]},\n",
    "            'effect': {'concept': relation[1]},\n",
    "        }, 'sources': sample['sources']}\n",
    "\n",
    "        list_graph.append(causal_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save List-Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonarray = json.dumps(list_graph)\n",
    "file_list_graph = open(PATH_OUTPUT_GRAPH, \"w+\")\n",
    "file_list_graph.write(jsonarray)\n",
    "file_list_graph.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cikm20-causenet",
   "language": "python",
   "name": "cikm20-causenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
